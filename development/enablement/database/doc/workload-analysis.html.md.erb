---
layout: handbook-page-toc
title: Workload analysis for GitLab.com
---

## On this page
{:.no_toc .hidden-md .hidden-lg}

- TOC
{:toc .hidden-md .hidden-lg}
## Workload Analysis for GitLab.com

This document discusses several approaches to understand the database workload for GitLab.com better. It aims to provide a few more perspectives on database workload, in addition to already existing monitoring solutions.

### Index bloat

In [previous studies](https://gitlab.com/gitlab-com/gl-infra/readiness/-/tree/master/library/database/postgres/bloat/#index-analysis), we've established that GitLab.com suffers a lot from bloat in btree indexes. That is, over time, some indexes tend to grow a lot beyond their ideal size - they take up more space than needed and become less efficient over time. The ideal size for an index is its most compact representation. This is the case when the index is built freshly, but regular updates to the index cannot maintain this compact representation over time in many cases.

For GitLab.com, we've previously introduced monitoring database bloat in good detail. This is based on a bloat heuristic for individual tables and indexes. Note this is an estimation only and the error is unbound - but it typically provides good insight into database bloat and is much cheaper than rebuilding indexes and tables fully (e.g. using `VACUUM FULL`) to determine their individual level of bloat. Thanks to this estimation, we've exposed metrics available through Prometheus and best seen in this [Grafana dashboard](https://dashboards.gitlab.net/d/000000224/postgresql-bloat?orgId=1&refresh=5m).

Aggregating those statistics, we can see that index bloat (yellow line) steadily grows over time while table bloat (green line) stays rather constant. The graph below shows about 3 months (early 2020) and an increase in index bloat from 240 GB to more than 600 GB.

![index-bloat-1](workload-analysis/index-bloat-1.png)

A few months later, in summer 2020, overall index bloat has accumulated to more than 1.3 TB (see below). Notice that table bloat increase significantly, too - however that was due to a single table which a data migration dealt with during that time (so that was expected). Compared to the overall database size at that time of 8.5 TB, index bloat made up about 15% of the total size. This space is mostly wasted and it occupies memory.

![index-bloat-3](workload-analysis/index-bloat-3.png)

We've started to address this through manual invocations of [pg_repack](https://gitlab.com/gitlab-com/gl-infra/readiness/-/tree/master/library/database/postgres/bloat/#design-1) in early September (the dip in index bloat and the vertical green lines indicate that). The overall impact of the autumn repacking efforts can be seen below. In multiple steps, we've successfully lowered index bloat to more acceptable levels - freeing up over 1 TB of space. After the data migration had finished, we also removed table bloat - again freeing up over 1 TB of additional space (the green line).

![index-bloat-2](workload-analysis/index-bloat-2.png)

##### Addressing index bloat long term

We came to realize that addressing index bloat is something we need to automate. This allows us to address this problem in a higher frequency, with no manual efforts and ultimately we maintain a healthy state of index bloat over time.

We also expect larger self-hosted installations of GitLab to suffer from this problem. Therefore, we developed a reindexing feature that ships with GitLab. Currently, this is being rolled out to GitLab.com and once proven successful, the feature will be available and enabled in GitLab by default.

The reindexing implementation is based on the fact that a [majority of index bloat stems from regular (non unique) btree indexes](https://gitlab.com/gitlab-com/gl-infra/readiness/-/tree/master/library/database/postgres/bloat/#index-analysis). Those can be recreated relatively easily, without risk of prolonged locking situations:

1. Create a new index with the same definition using a temporary name (index creation using `CONCURRENTLY` option)
2. Swap original index with newly created one
3. Drop the original index

Now this approach is not possible for indexes supporting primary keys. Fortunately, this is not where we need to deal with a lot of index bloat in any case - so we don't need to solve this problem right now.

Note that once GitLab requires PostgreSQL 12, we'll be able to leverage the [concurrent reindexing feature](https://paquier.xyz/postgresql-2/postgres-12-reindex-concurrently/) introduced there. Furthermore, PostgreSQL 12 has seen [significant improvements in terms of btree storage savings](https://www.cybertec-postgresql.com/en/b-tree-index-improvements-in-postgresql-v12/), which we expect to benefit from eventually, too.

The reindexing lives inside a Rake task that gets triggered through a cronjob. For GitLab.com, we expect to be able to maintain healthy levels of index bloat by running this task on weekends only.

Going forward and with %13.6, we are going to improve this feature by adding a [good index selection strategy](https://gitlab.com/gitlab-org/gitlab/-/issues/258576) and [observability features](https://gitlab.com/gitlab-org/gitlab/-/issues/263463) to it.

###### Relevant links

1. [Automatic reindexing epic](https://gitlab.com/groups/gitlab-org/-/epics/3989)
2. [Database bloat dashboard](https://dashboards.gitlab.net/d/000000224/postgresql-bloat?orgId=1&refresh=5m)
3. [Index bloat study (2019)](https://gitlab.com/gitlab-com/gl-infra/readiness/-/tree/master/library/database/postgres/bloat/#index-analysis)

### High frequency top-k query analysis based on pg_stat_statements



pg_stat_statements and Marginalia

pgBadger